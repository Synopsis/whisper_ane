{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16f71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4e3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "import numpy as np\n",
    "import coremltools as ct\n",
    "\n",
    "from pathlib import Path\n",
    "from whisper_ane.arch.encoder import AudioEncoderANE\n",
    "from whisper_ane.export.utils import *\n",
    "\n",
    "torch.set_printoptions(4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a17c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT ARGS -- TO BE INPUT BY USER\n",
    "ARCH = \"base\"\n",
    "BASE_EXPORT_DIR = f\"/Users/rahulsomani/Desktop/Whisper-ANE-Encoder/{ARCH}\"\n",
    "# COMPUTE_PRECISION = ct.precision.FLOAT16\n",
    "COMPUTE_PRECISION = ct.precision.FLOAT32\n",
    "# COMPUTE_PRECISION = None\n",
    "MIN_DEPLOYMENT_TARGET = None # ct.target.macOS12\n",
    "\n",
    "# Input / output names\n",
    "AUDIO_INPUT_NAME = \"logmel_data\"\n",
    "ENCODER_OUTPUT_NAME = \"encoded_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "383c0c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checks and stuff\n",
    "EXPORT_SUFFIX = f\"--precision-fp{16 if COMPUTE_PRECISION is ct.precision.FLOAT16 else 32}\"\n",
    "\n",
    "_AVAILABLE_ARCHS = whisper.available_models()\n",
    "if not ARCH in _AVAILABLE_ARCHS:\n",
    "    raise RuntimeError(\n",
    "        f\"Selected arch '{ARCH}' is not available. Available options are: {_AVAILABLE_ARCHS}\"\n",
    "    )\n",
    "BASE_EXPORT_DIR = Path(BASE_EXPORT_DIR)\n",
    "BASE_EXPORT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770803a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick utils\n",
    "permute_dims = (0, 3, 1, 2)\n",
    "tfm = lambda x: x.permute(permute_dims).squeeze(-1)\n",
    "def abs_diff(x1, x2):\n",
    "    return (x1 - x2).abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5b4d5",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f87c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(ARCH)\n",
    "x = torch.rand(1, 80, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c4764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_stock = model.encoder\n",
    "encoder_ane = AudioEncoderANE.from_stock_encoder(encoder_stock)\n",
    "\n",
    "encoder_stock.eval();\n",
    "encoder_ane.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ca41db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1500, 512]), torch.Size([1, 512, 1, 1500]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out_orig = encoder_stock(x)\n",
    "    out_ane = encoder_ane(x)\n",
    "\n",
    "out_orig.shape, out_ane.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "944a1877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Abs Diff: ', tensor(2.8798))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('Cos Sim:  ', tensor(1.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Abs Diff: \", abs_diff(out_orig, tfm(out_ane))\n",
    "\"Cos Sim:  \", torch.cosine_similarity(out_orig, tfm(out_ane)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b09bc",
   "metadata": {},
   "source": [
    "### JIT Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d7b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulsomani/git/ml-ane-transformers/ane_transformers/reference/layer_norm.py:60: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert inputs.size(1) == self.num_channels\n",
      "/Users/rahulsomani/git/whisper-ane/whisper_ane/arch/multihead_attention.py:41: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert x.shape[2] == 1, f\"Expected third dim to be 1. Got {x.shape[2]} instead (full shape -> {x.shape}).\"\n",
      "/Users/rahulsomani/git/whisper-ane/whisper_ane/arch/multihead_attention.py:91: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_per_head = dim // self.n_head\n",
      "/Users/rahulsomani/git/whisper-ane/whisper_ane/arch/multihead_attention.py:104: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  normalize_factor = float(dim_per_head) ** -0.5\n",
      "/Users/rahulsomani/git/whisper/whisper/model.py:154: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
      "/Users/rahulsomani/git/whisper/whisper/model.py:90: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  scale = (n_state // self.n_head) ** -0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.1 s, sys: 613 ms, total: 30.7 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoder_ane_jit = torch.jit.trace(encoder_ane, x)\n",
    "encoder_stock_jit = torch.jit.trace(encoder_stock, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d51d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.37 s, sys: 121 ms, total: 6.49 s\n",
      "Wall time: 5.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out_ane_jit = encoder_ane_jit(x)\n",
    "out_stock_jit = encoder_stock_jit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185c6025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PyTorch jitted stock - PyTorch jitted ANE',\n",
       " tensor(2.8798, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"PyTorch jitted stock - PyTorch jitted ANE\", abs_diff(out_stock_jit, tfm(out_ane_jit))\n",
    "torch.allclose(out_stock_jit, tfm(out_ane_jit), atol=1e-2)\n",
    "torch.allclose(out_ane_jit, out_ane)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634a530",
   "metadata": {},
   "source": [
    "### CoreML Util - Add Metadata To Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "512d1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def add_metadata_for_encoder(\n",
    "    encoder: ct.models.MLModel,\n",
    "    input_name: str = 'logmel_data',\n",
    "    output_name: str = 'audio_embedding',\n",
    "    output_shape: List[int] = None,\n",
    ") -> ct.models.MLModel:\n",
    "    assert output_shape, f\"`output_shape` required\"\n",
    "    spec = encoder.get_spec()\n",
    "\n",
    "    # Add top level metadata\n",
    "    spec.description.metadata.author = \"OpenAI / Ozu\"  # ...?\n",
    "    spec.description.metadata.license = \"MIT\"\n",
    "    spec.description.metadata.shortDescription = f\"\"\"\n",
    "    '{ARCH}' variant of OpenAI's Whisper (https://github.com/openai/whisper) optimised for the ANE using the principles outlined in Apple's repo (https://github.com/apple/ml-ane-transformers)\n",
    "    \"\"\".replace('\\n', ' ')\n",
    "\n",
    "    output = encoder.predict({input_name: x.numpy()})\n",
    "    assert len(output.keys()) == 1\n",
    "\n",
    "    {k:v.shape for k,v in output.items()}\n",
    "\n",
    "    # rename output feature\n",
    "    ct.utils.rename_feature(spec, list(output.keys())[0], output_name)\n",
    "\n",
    "    # add more metadata about inputs & outputs\n",
    "    input_type = find_io_type(spec, input_name, search_inputs=True)\n",
    "    input_type.shortDescription = \"Mel spectogram audio input\"\n",
    "\n",
    "    output_type = find_io_type(spec, output_name, search_outputs=True)\n",
    "    output_type.type.multiArrayType.shape.extend(output_shape)\n",
    "    output_type.shortDescription = 'Audio embeddings in the shape (BS, embed_dim, 1, seq_len)'\n",
    "\n",
    "    return ct.models.MLModel(spec, weights_dir=encoder.weights_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb9118b",
   "metadata": {},
   "source": [
    "### Export Stock Model To CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96c4e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|███████████████████████████████████▉| 531/532 [00:00<00:00, 3071.58 ops/s]\n",
      "Running MIL Common passes:   0%|                                                            | 0/40 [00:00<?, ? passes/s]/Users/rahulsomani/miniconda3/envs/rosetta/lib/python3.8/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:135: UserWarning: Output, '717', of the source model, has been renamed to 'var_717' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL Common passes: 100%|██████████████████████████████████████████████████| 40/40 [00:00<00:00, 175.62 passes/s]\n",
      "Running MIL Clean up passes: 100%|████████████████████████████████████████████████| 11/11 [00:00<00:00, 108.89 passes/s]\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "_encoder_stock_mlmodel = ct.convert(\n",
    "    encoder_stock_jit,\n",
    "    minimum_deployment_target = MIN_DEPLOYMENT_TARGET,\n",
    "    compute_precision = COMPUTE_PRECISION,\n",
    "    convert_to=\"mlprogram\",\n",
    "    inputs = [ct.TensorType(name=AUDIO_INPUT_NAME, shape=x.shape)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ebd0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_stock_mlmodel = add_metadata_for_encoder(\n",
    "         encoder = _encoder_stock_mlmodel,\n",
    "      input_name = AUDIO_INPUT_NAME,\n",
    "     output_name = ENCODER_OUTPUT_NAME,\n",
    "    output_shape = list(out_stock_jit.shape)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01c62f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1168, -0.1706, -0.4942,  ..., -0.8114, -0.5475,  0.0136],\n",
       "         [ 0.6642, -0.0579,  0.0812,  ..., -0.1821, -0.1616,  0.0357],\n",
       "         [ 1.2810,  0.3786, -0.1048,  ..., -0.3349, -0.8647, -0.2195],\n",
       "         ...,\n",
       "         [-0.6410, -0.6419,  0.7206,  ...,  0.3759,  0.2229,  1.0623],\n",
       "         [-0.3861, -0.3299, -0.6774,  ...,  0.1726,  0.8344,  0.2988],\n",
       "         [-1.0575,  0.8441, -0.0095,  ...,  0.5406, -0.3654,  0.1805]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_stock_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cc66f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoded_audio': array([[[ 0.11680819, -0.1705755 , -0.4942226 , ..., -0.8114355 ,\n",
       "          -0.54749817,  0.01363501],\n",
       "         [ 0.6641544 , -0.05793391,  0.08117322, ..., -0.18210214,\n",
       "          -0.16157283,  0.03564826],\n",
       "         [ 1.280986  ,  0.37862337, -0.10479611, ..., -0.3349134 ,\n",
       "          -0.86473936, -0.21953698],\n",
       "         ...,\n",
       "         [-0.6410045 , -0.64186215,  0.7205772 , ...,  0.37588137,\n",
       "           0.22286299,  1.0622998 ],\n",
       "         [-0.38605604, -0.32986856, -0.6774049 , ...,  0.17257194,\n",
       "           0.8343568 ,  0.29884678],\n",
       "         [-1.0575485 ,  0.8441242 , -0.00949121, ...,  0.54062414,\n",
       "          -0.36536804,  0.18053477]]], dtype=float32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_stock_mlmod = encoder_stock_mlmodel.predict(\n",
    "    {AUDIO_INPUT_NAME: x.numpy()}\n",
    ")\n",
    "out_stock_mlmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0696939d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PyTorch jitted stock - coreml stock', tensor(4.0219, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output = torch.from_numpy(out_stock_mlmod[ENCODER_OUTPUT_NAME])\n",
    "\"PyTorch jitted stock - coreml stock\", abs_diff(_output, out_stock_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab0edf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_stock_jit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc5c147e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(_output, out_stock_jit, dim=1).mean()\n",
    "torch.cosine_similarity(_output, out_stock_jit, dim=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7c517a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_stock_mlmodel.save(BASE_EXPORT_DIR / f\"whisper-{ARCH}-stock{EXPORT_SUFFIX}.mlpackage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e737909",
   "metadata": {},
   "source": [
    "### Export ANE Model To CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1008ed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|███████████████████████████████████▉| 892/893 [00:00<00:00, 7905.20 ops/s]\n",
      "Running MIL Common passes:   0%|                                                            | 0/40 [00:00<?, ? passes/s]/Users/rahulsomani/miniconda3/envs/rosetta/lib/python3.8/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:135: UserWarning: Output, '1192', of the source model, has been renamed to 'var_1192' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL Common passes: 100%|██████████████████████████████████████████████████| 40/40 [00:00<00:00, 137.13 passes/s]\n",
      "Running MIL Clean up passes: 100%|████████████████████████████████████████████████| 11/11 [00:00<00:00, 113.65 passes/s]\n"
     ]
    }
   ],
   "source": [
    "_encoder_ane_mlmodel = ct.convert(\n",
    "    encoder_ane_jit,\n",
    "    minimum_deployment_target = MIN_DEPLOYMENT_TARGET,\n",
    "    compute_precision = COMPUTE_PRECISION,\n",
    "    convert_to = \"mlprogram\",\n",
    "    inputs = [ct.TensorType(name=AUDIO_INPUT_NAME, shape=x.shape)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44340ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ane_mlmodel = add_metadata_for_encoder(\n",
    "         encoder = _encoder_ane_mlmodel,\n",
    "      input_name = AUDIO_INPUT_NAME,\n",
    "     output_name = ENCODER_OUTPUT_NAME,\n",
    "    output_shape = list(out_ane_jit.shape)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a765f0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1168,  0.6642,  1.2810,  ..., -0.6410, -0.3861, -1.0575]],\n",
       "\n",
       "         [[-0.1706, -0.0579,  0.3786,  ..., -0.6419, -0.3299,  0.8441]],\n",
       "\n",
       "         [[-0.4942,  0.0812, -0.1048,  ...,  0.7206, -0.6774, -0.0095]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.8114, -0.1821, -0.3349,  ...,  0.3759,  0.1726,  0.5406]],\n",
       "\n",
       "         [[-0.5475, -0.1616, -0.8647,  ...,  0.2229,  0.8344, -0.3654]],\n",
       "\n",
       "         [[ 0.0136,  0.0357, -0.2195,  ...,  1.0623,  0.2988,  0.1805]]]],\n",
       "       grad_fn=<DifferentiableGraphBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out_ane_jit\n",
    "encoder_ane_jit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4155fd49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoded_audio': array([[[[ 0.11680963,  0.6641547 ,  1.2809876 , ..., -0.64100677,\n",
       "           -0.38605592, -1.057548  ]],\n",
       " \n",
       "         [[-0.17057452, -0.05793391,  0.37862483, ..., -0.6418604 ,\n",
       "           -0.32986853,  0.8441241 ]],\n",
       " \n",
       "         [[-0.49422303,  0.08117198, -0.10479528, ...,  0.72057784,\n",
       "           -0.67740405, -0.00949142]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.81143546, -0.18210167, -0.33491337, ...,  0.37588137,\n",
       "            0.17257176,  0.5406247 ]],\n",
       " \n",
       "         [[-0.5474986 , -0.16157244, -0.8647394 , ...,  0.22286098,\n",
       "            0.83435595, -0.36536828]],\n",
       " \n",
       "         [[ 0.01363305,  0.03564599, -0.21953641, ...,  1.0623014 ,\n",
       "            0.29884604,  0.18053426]]]], dtype=float32)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ane_mlmod = encoder_ane_mlmodel.predict({AUDIO_INPUT_NAME: x.numpy()})\n",
    "out_ane_mlmod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1c054",
   "metadata": {},
   "source": [
    "Stock PT output is the GT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92ec4ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PyTorch jitted ANE - coreml ANE', tensor(4.9226, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_ane = torch.from_numpy(out_ane_mlmod[ENCODER_OUTPUT_NAME])\n",
    "\"PyTorch jitted ANE - coreml ANE\", abs_diff(_output_ane, out_ane_jit)\n",
    "# torch.cosine_similarity(torch.from_numpy(out_ane_mlmod[\"var_1192\"]), out_ane_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6728313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 1, 1500])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ane_jit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3b6f5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(_output_ane, out_ane_jit, dim=1).mean()\n",
    "torch.cosine_similarity(_output_ane, out_ane_jit, dim=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7d914",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1b4fc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9007, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stock vs ANE MLModel diff\n",
    "abs_diff(_output_ane, out_ane_jit) - abs_diff(_output, out_stock_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57ffdee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('coreml stock - coreml ANE', tensor(1.1897))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"coreml stock - coreml ANE\", abs_diff(tfm(_output_ane), _output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d13b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ane_mlmodel.save(BASE_EXPORT_DIR / f\"whisper-{ARCH}-ane{EXPORT_SUFFIX}.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeedbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e44e80c9b10595451fd00a84307f42b52d261df5869589114361b2e6c7d43758"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
